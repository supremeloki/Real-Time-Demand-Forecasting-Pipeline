{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapp Real-Time Ride Demand Forecasting: Portfolio Demo\n",
    "\n",
    "This notebook demonstrates the end-to-end Machine Learning Operations (MLOps) pipeline for real-time ride demand forecasting, a core component for platforms like Snapp.\n",
    "\n",
    "**Project Goal**: Predict future ride demand in specific geographical grids (e.g., 1x1 km) for upcoming time intervals (e.g., next 15 minutes) to optimize driver allocation, dynamic pricing, and overall operational efficiency.\n",
    "\n",
    "## 1. Environment Setup and Configuration\n",
    "We begin by loading our project configuration and setting up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import mlflow\n",
    "import time\n",
    "\n",
    "# Ensure the project root is in the path for module imports\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "    sys.path.insert(0, os.path.abspath('../src'))\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('.'))\n",
    "    sys.path.insert(0, os.path.abspath('./src'))\n",
    "\n",
    "from src.utils.config_reader import ConfigReader\n",
    "from src.utils.logging_setup import setup_logging\n",
    "from src.data_artifacts.tehran_traffic_simulator.generate_snapp_data import SnappDataGenerator\n",
    "from src.batch_processing.batch_feature_creation import BatchFeatureCreator\n",
    "from src.stream_analytics.spark_stream_features import SparkStreamFeatures # Note: Requires Spark environment\n",
    "from src.model_training.train import ModelTrainer\n",
    "from src.model_serving.prediction_logic import PredictionLogic\n",
    "from model_experiments.ab_testing.lono_interface import LonoABTester\n",
    "from model_experiments.monitoring.data_drift_detector import DataDriftDetector\n",
    "from model_experiments.monitoring.model_performance_monitor import ModelPerformanceMonitor\n",
    "\n",
    "logger = setup_logging(\"portfolio_demo\")\n",
    "config = ConfigReader(env=\"dev\")\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "# Set MLflow tracking URI for local demo (if not already set via env var)\n",
    "mlflow.set_tracking_uri(config.get(\"model_registry.uri\"))\n",
    "mlflow.set_experiment(f\"{config.get('model_registry.model_name')}_experiment\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Simulation & Feature Engineering\n",
    "We'll simulate real-time ride events for Tehran and then generate both batch and stream-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Data\n",
    "print(\"\\n--- Generating Synthetic Raw Data ---\")\n",
    "start_sim = datetime.now() - timedelta(days=5)\n",
    "end_sim = datetime.now() - timedelta(hours=1) # simulate historical data up to a recent point\n",
    "generator = SnappDataGenerator(start_sim, end_sim)\n",
    "synthetic_raw_data_df = generator.generate_events(num_events_per_15min=25)\n",
    "synthetic_raw_data_path = \"data_artifacts/sample_data/synthetic_snapp_data.csv\"\n",
    "synthetic_raw_data_df.to_csv(synthetic_raw_data_path, index=False)\n",
    "print(f\"Generated {len(synthetic_raw_data_df)} synthetic events and saved to {synthetic_raw_data_path}\")\n",
    "print(synthetic_raw_data_df.head())\n",
    "\n",
    "# Batch Feature Creation\n",
    "print(\"\\n--- Running Batch Feature Creation ---\")\n",
    "batch_feature_creator = BatchFeatureCreator(env=\"dev\")\n",
    "batch_features_df = batch_feature_creator.generate_batch_features()\n",
    "batch_features_path = \"data_artifacts/sample_data/batch_processed_features.csv\"\n",
    "batch_features_df.to_csv(batch_features_path, index=False)\n",
    "print(f\"Generated {len(batch_features_df)} batch features and saved to {batch_features_path}\")\n",
    "print(batch_features_df.head())\n",
    "\n",
    "# Stream Feature Creation (Conceptual - requires a running Spark/Kafka cluster, demonstrating with an empty run)\n",
    "print(\"\\n--- Simulating Stream Feature Creation (Conceptual) ---\")\n",
    "print(\"To run real stream features, ensure Kafka and Spark are running and uncomment the code below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training & MLflow Integration\n",
    "Train our XGBoost model using the batch-processed features and log everything with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training Model with MLflow ---\")\n",
    "model_trainer = ModelTrainer(env=\"dev\")\n",
    "model_trainer.train_model()\n",
    "\n",
    "print(\"Model training complete. Check MLflow UI for experiment results.\")\n",
    "\n",
    "# Promote the latest model to 'Production' stage (simulated)\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "model_name = config.get(\"model_registry.model_name\")\n",
    "try:\n",
    "    latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[0]\n",
    "    client.transition_model_version_stage(\n",
    "        name=latest_version.name,\n",
    "        version=latest_version.version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "    print(f\"Latest model version {latest_version.version} transitioned to Production stage.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not transition model to Production (perhaps no model registered yet, or it's already in Production): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-time Model Serving (FastAPI Simulation)\n",
    "Demonstrate interaction with the deployed model serving API for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Simulating Model Serving API Interaction ---\")\n",
    "\n",
    "try:\n",
    "    prediction_logic = PredictionLogic(env=\"dev\")\n",
    "    print(\"PredictionLogic initialized, model loaded.\")\n",
    "\n",
    "    # Prepare a sample feature set for prediction\n",
    "    sample_features_df = batch_features_df.sample(1, random_state=42)\n",
    "    sample_features = sample_features_df[prediction_logic.feature_columns].iloc[0].to_dict()\n",
    "    actual_demand = sample_features_df[config.get(\"features_params.target_variable\")].iloc[0]\n",
    "    \n",
    "    print(\"Sample features:\", sample_features)\n",
    "    \n",
    "    predicted_demand = prediction_logic.predict_demand(sample_features)\n",
    "    print(f\"Predicted demand: {predicted_demand:.2f}, Actual demand: {actual_demand:.2f}\")\n",
    "\n",
    "    # Batch Prediction\n",
    "    sample_features_list = batch_features_df.sample(5, random_state=100)[prediction_logic.feature_columns].to_dict(orient='records')\n",
    "    batch_predictions = prediction_logic.batch_predict_demand(sample_features_list)\n",
    "    print(\"Batch predictions (first 5):\")\n",
    "    for i, pred in enumerate(batch_predictions):\n",
    "        print(f\"  Prediction {i+1}: {pred:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during model serving simulation: {e}\")\n",
    "    print(\"Please ensure the MLflow registry has a model in 'Production' stage and paths are correct.\")\n",
    "    print(\"For a full demo, run `uvicorn src.model_serving.api_app:app --reload` in a separate terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A/B Testing with LONO (Simulated)\n",
    "Demonstrate how `LonoABTester` would route requests to different model versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Simulating A/B Testing with LONO ---\")\n",
    "ab_tester = LonoABTester(env=\"dev\")\n",
    "\n",
    "# Create a dummy A/B test config file if it doesn't exist for the demo\n",
    "ab_test_config_path = \"conf/ab_test_rules.json\"\n",
    "if not os.path.exists(ab_test_config_path):\n",
    "    with open(ab_test_config_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"champion_model_version\": \"Production\",\n",
    "            \"challenger_model_version\": \"Staging\",\n",
    "            \"challenger_traffic_percentage\": 0.3 # 30% to challenger\n",
    "        }, f)\n",
    "    print(\"Created dummy A/B test config for demo.\")\n",
    "\n",
    "sample_features_for_ab = batch_features_df.sample(1, random_state=50)[prediction_logic.feature_columns].iloc[0].to_dict()\n",
    "\n",
    "print(\"Making 10 A/B tested predictions:\")\n",
    "for i in range(10):\n",
    "    pred = ab_tester.get_prediction(sample_features_for_ab)\n",
    "    print(f\"  Request {i+1}: Predicted demand = {pred:.2f}\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Clean up dummy config\n",
    "if os.path.exists(ab_test_config_path):\n",
    "    os.remove(ab_test_config_path)\n",
    "    print(f\"Cleaned up {ab_test_config_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Monitoring (Data Drift & Performance)\n",
    "Demonstrate how drift and performance monitoring would be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Data Drift Detection ---\")\n",
    "drift_detector = DataDriftDetector(env=\"dev\")\n",
    "drift_report = drift_detector.run_drift_detection_pipeline()\n",
    "print(\"Drift Report:\\n\", json.dumps(drift_report, indent=4))\n",
    "\n",
    "print(\"\\n--- Running Model Performance Monitoring ---\")\n",
    "performance_monitor = ModelPerformanceMonitor(env=\"dev\")\n",
    "performance_report = performance_monitor.run_performance_monitoring_pipeline()\n",
    "print(\"Performance Report:\\n\", json.dumps(performance_report, indent=4))\n",
    "\n",
    "print(\"\\nDemo complete. Explore the `model_experiments/monitoring` directory for generated reports.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
